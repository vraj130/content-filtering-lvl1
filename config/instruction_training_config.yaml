# Training Configuration for Content Filtering LV1 - Instruction Fine-Tuning
# Stage 1: High-recall AI content detection using instruction-based approach

# Model Configuration
model:
  hugging_face_model_id: "google/gemma-3-1b-it"
  max_length: 8192
  attention_implementation: "flash_attention_2"

quick_test: false
# Class Labels
class_mapping:
  nonai: 0
  ai: 1

# Instruction Prompting Configuration
instruction:
  prompt_template: |
    Does the following text include anything related to AI or LLMs?
    If so, output 'Yes'. Otherwise, output 'No'.

    <text>
    {text}
    </text>

    Remember to only output a single token: 'Yes' or 'No'.
    
  answer_yes: "Yes"
  answer_no: "No"

# GPU Configuration
gpu:
  graphic_card: "0"
  gpu_device: "cuda:0"

# Quantization Configuration
quantization:
  load_in_4bit: false
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"

# LoRA Configuration
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.2
  bias: "none"
  task_type: "CAUSAL_LM" 
  target_modules:
    - "gate_proj"
    - "down_proj"
    - "v_proj"
    - "k_proj"
    - "q_proj"
    - "o_proj"
    - "up_proj"

wandb:
  enabled: true
  project: "gemma-chat-template"
  run_name: "gemma-v1-1116"
  api_key: ${WANDB_API_KEY}

hf_hub:
  repo_id: "metr-evals/content-filtering-models"
  repo_type: "model"
  branch: "gemma-v1-chat-template-1116"
  token: ${HF_TOKEN}  # Optional: use environment variable for authentication


# Training Arguments
training:
  checkpoint_name: "gemma-v1-chat-template-1116"
  output_dir: "outputs/checkpoints/gemma-v1-chat-template-1116/checkpoints"
  learning_rate: 2.0e-4  
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  num_train_epochs: 1
  weight_decay: 0.01
  eval_strategy: "steps"
  eval_steps: 796
  save_strategy: "steps"
  save_steps: 398
  load_best_model_at_end: true
  push_to_hub: true
  logging_strategy: "steps"
  logging_steps: 25
  gradient_accumulation_steps: 4
  bf16: true
  fp16: false
  warmup_ratio: 0.1
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  gradient_checkpointing: true
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2


# Output Configuration
output:
  model_dir: "outputs/checkpoints/gemma-v1-chat-template-1116/final_model"
  tensorboard_dir: "outputs/tensorboard/gemma-v1-chat-template-1116"


# Early Stopping
early_stopping:
  patience: 1
  threshold: 0.001


# Data Configuration
data:
  data_dir: "/workspace/content-filtering-lvl1/data/final_data_4_v2/train_all_data"
  train_file: "train.parquet"
  val_file: "val.parquet"
  test_file: "test.parquet"

# Weights & Biases Configuration

# Slack Notifications
slack:
  enabled: true
  webhook_url: ${SLACK_WEBHOOK_URL}